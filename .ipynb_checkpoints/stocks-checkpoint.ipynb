{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Trades by Members of the US House of Representatives\n",
    "\n",
    "* **See the main project notebook for instructions to be sure you satisfy the rubric!**\n",
    "* See Project 03 for information on the dataset.\n",
    "* A few example prediction questions to pursue are listed below. However, don't limit yourself to them!\n",
    "    - Can you predict the party affiliation of a representative from their stock trades?\n",
    "    - Can you predict the geographic region that the representative comes from using their stock trades? E.g., west coast, east coast, south, etc.\n",
    "    * Can you predict whether a particular trade is a BUY or SELL?\n",
    "\n",
    "Be careful to justify what information you would know at the \"time of prediction\" and train your model using only those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "\n",
    "### Introduction\n",
    "In this notebook, we predict the party affiliation of a representative from their stock trades. In order to do so, we collected data from [https://housestockwatcher.com/api](https://housestockwatcher.com/api) and combined it with [https://www.kaggle.com/datasets/unanimad/us-election-2020](https://www.kaggle.com/datasets/unanimad/us-election-2020) in order to find the party membership of the candidates. Overall, we had the four columns 'transaction_date', 'amount', 'ticker' and 'type' at our disposition in order to predict the party membership of associated politician. Therefore we have a classification problem. Our assumption for this to work is that democrats, on average, trade differently than republicans.\n",
    "\n",
    "### Baseline Model\n",
    "The baseline model will be a DecisionTreeClassifier trained on the either raw or One-Hot-encoded features. On a test dataset being 20% the size of the total data and 100 iterations, the baseline model attains an average mean accuracy of 0.79. This gives us a value on which we can base the benchmarks of the further analysis.\n",
    "\n",
    "### Final Model\n",
    "In order to improve the baseline model, we took various measures:\n",
    "- Transform the 'amount' variable into an integer and standardize it based on the transaction type\n",
    "- Put the 'transaction_date' into One-Hot-encoded bins. Divide the 3 years into 12 bins in order to represent business quarters.\n",
    "- Use the support vector classifier SVC and systematically adapt the hyperparameter C to yield the best results.\n",
    "\n",
    "### Fairness Evaluation\n",
    "- We realised that the dataset contains more transactions effected by republicans which leads the model to predict the label \"REP\" with  for a transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfwp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import fuzz\n",
    "import fuzzywuzzy.process as fwp\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from stdscaler import StdScalerByGroup\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdScalerByGroup(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        :Example:\n",
    "        >>> cols = {'g': ['A', 'A', 'B', 'B'], 'c1': [1, 2, 2, 2], 'c2': [3, 1, 2, 0]}\n",
    "        >>> X = pd.DataFrame(cols)\n",
    "        >>> std = StdScalerByGroup().fit(X)\n",
    "        >>> std.grps_ is not None\n",
    "        True\n",
    "        \"\"\"\n",
    "        # X might not be a pandas DataFrame (e.g. a np.array)\n",
    "        df = pd.DataFrame(X)\n",
    "\n",
    "        # Compute and store the means/standard-deviations for each column (e.g. 'c1' and 'c2'),\n",
    "        # for each group (e.g. 'A', 'B', 'C').\n",
    "        # (Our solution uses a dictionary)\n",
    "        self.groups = df[df.columns.tolist()[0]].unique().tolist()\n",
    "        self.indizes = df.columns.tolist()[1:]\n",
    "        self.grp_identifier = df.columns.tolist()[0]\n",
    "        self.grps_ = df.groupby(self.grp_identifier).agg([\"std\", \"mean\"])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        :Example:\n",
    "        >>> cols = {'g': ['A', 'A', 'B', 'B'], 'c1': [1, 2, 3, 4], 'c2': [1, 2, 3, 4]}\n",
    "        >>> X = pd.DataFrame(cols)\n",
    "        >>> std = StdScalerByGroup().fit(X)\n",
    "        >>> out = std.transform(X)\n",
    "        >>> out.shape == (4, 2)\n",
    "        True\n",
    "        >>> np.isclose(out.abs(), 0.707107, atol=0.001).all().all()\n",
    "        True\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            getattr(self, \"grps_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must fit the transformer before tranforming the data!\")\n",
    "\n",
    "        # Hint: Define a helper function here!\n",
    "\n",
    "        df = pd.DataFrame(X)\n",
    "        for group in self.groups:\n",
    "            for index in self.indizes:\n",
    "                df.loc[df[self.grp_identifier] == group, index] = (df.loc[df[self.grp_identifier] == group, index] -\n",
    "                                                                   self.grps_[index, \"mean\"][group]) / \\\n",
    "                                                                  self.grps_[index, \"std\"][group]\n",
    "        return df.drop(self.grp_identifier, axis=1).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parties(path_cand='./data/house_candidate.csv', path_trans='./data/all_transactions.csv'):\n",
    "    data_candidates = pd.read_csv(path_cand)\n",
    "    data_candidates[\"candidate\"] = data_candidates[\"candidate\"].str.lower()\n",
    "    data_transactions = pd.read_csv(path_trans)\n",
    "    data_transactions[\"representative\"] = data_transactions[\"representative\"]\\\n",
    "        .str.lower()\\\n",
    "        .str.replace(\"hon. \", \"\", regex=False)\n",
    "    data_transactions[\"amount\"] = data_transactions[\"amount\"]\\\n",
    "        .str.replace(\"$\", \"\", regex=False)\\\n",
    "        .str.replace(\" \", \"\", regex=False)\\\n",
    "        .str.replace(\",\", \"\", regex=False)\\\n",
    "        .str.split(\"-\")\\\n",
    "        .apply(lambda x : np.floor(pd.to_numeric(pd.Series(x), errors=\"coerce\").mean()))\n",
    "    data_transactions.loc[data_transactions[\"ticker\"] == \"--\", \"ticker\"] = np.NaN\n",
    "    choices = data_transactions[\"representative\"].unique().tolist()\n",
    "    data_candidates[\"representative\"] = data_candidates[\"candidate\"].apply(lambda x : fmatch(x, choices))\n",
    "    data_candidates = data_candidates[[\"representative\", \"party\"]].drop_duplicates().dropna()\n",
    "    data_candidates = remove_duplicate_name(data_candidates)\n",
    "    data_transactions = data_transactions[[\"transaction_date\", \"ticker\", \"type\", \"amount\", \"representative\"]]\\\n",
    "        .drop_duplicates()\n",
    "    return data_transactions.merge(data_candidates, how=\"left\", left_on=\"representative\", right_on=\"representative\")\n",
    "\n",
    "def fmatch(row, choices):\n",
    "    choice = fwp.extractOne(row, choices, scorer=fuzz.token_set_ratio, score_cutoff=81)\n",
    "    return np.NaN if choice is None else choice[0]\n",
    "\n",
    "def count_names(search, list):\n",
    "    return len(list[list[\"representative\"] == search]) if not search == np.NaN else np.NaN\n",
    "\n",
    "def remove_duplicate_name(df):\n",
    "    new_df = df.copy()\n",
    "    unique_names = pd.DataFrame(new_df[\"representative\"]\n",
    "                                .apply(lambda x : x if count_names(x, new_df) == 1 else np.NaN).dropna())\n",
    "    return unique_names.merge(df, how=\"left\", left_on=\"representative\", right_on=\"representative\", suffixes=(\"l\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = merge_parties()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = merge_parties()\n",
    "stocks = stocks.dropna()\n",
    "stocks = stocks[stocks[\"transaction_date\"].str[:5].apply(lambda x: x in [\"2019-\", \"2020-\", \"2021-\", \"2022-\"])]\n",
    "\n",
    "X = stocks[['type', 'amount', 'transaction_date', 'ticker']]\n",
    "y = stocks[[\"party\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = merge_parties().dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "year_prefix_hotencode = Pipeline([\n",
    "    (\"get-year\", FunctionTransformer(lambda x : pd.DataFrame(x[\"transaction_date\"].str[:4]))),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "transformers=[\n",
    "    ('amount', OneHotEncoder(handle_unknown='ignore'), [\"amount\", \"type\", \"ticker\"]),\n",
    "    ('year', year_prefix_hotencode, [\"transaction_date\"])\n",
    "]\n",
    ")\n",
    "pl = Pipeline([\n",
    "    ('preproc', preproc),\n",
    "    ('embedding', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "pl.fit(X_train, y_train[\"party\"])\n",
    "(pl.score(X_train, y_train[\"party\"]), pl.score(X_test, y_test[\"party\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\"test\":[],\"train\":[]}\n",
    "for _ in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(stocks[['type', 'amount', 'transaction_date', 'ticker']],\n",
    "                                                        stocks[['party']])\n",
    "    pl.fit(X_train, y_train[\"party\"])\n",
    "    res[\"test\"].append(pl.score(X_test, y_test[\"party\"]))\n",
    "    res[\"train\"].append(pl.score(X_train, y_train[\"party\"]))\n",
    "\n",
    "pd.DataFrame(res).plot.hist(bins=np.linspace(0.5, 1, 100),alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "- using SVC (State Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "year_prefix_hotencode = Pipeline([\n",
    "    (\"transform-date-to-int\", FunctionTransformer(lambda x : x[\"transaction_date\"].apply(lambda x : time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%d\").timetuple())))),\n",
    "    (\"to-2d-df\", FunctionTransformer(lambda x : pd.DataFrame(x))),\n",
    "    (\"bins\", KBinsDiscretizer()),\n",
    "])\n",
    "preproc = ColumnTransformer(\n",
    "transformers=[\n",
    "    ('amount', OneHotEncoder(handle_unknown='ignore'), [\"amount\", \"type\", \"ticker\"]),\n",
    "    ('year', year_prefix_hotencode, [\"transaction_date\"]),\n",
    "    ('stdscalerbygroup', StdScalerByGroup(), [\"type\", \"amount\"])\n",
    "]\n",
    ")\n",
    "pl = Pipeline([\n",
    "    ('preproc', preproc),\n",
    "    ('embedding', SVC()),\n",
    "])\n",
    "for x in range(25):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y.values.ravel(), random_state=x)\n",
    "    pl.fit(X_train, y_train)\n",
    "    res.append(pl.score(X_test, y_test))\n",
    "pd.Series(res).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\"n\":[],\"test\":[],\"train\":[]}\n",
    "for n in np.arange(0.5,8,0.3):\n",
    "    pl = Pipeline([\n",
    "        ('preproc', preproc),\n",
    "        ('embedding', SVC(C=n)),\n",
    "    ])\n",
    "\n",
    "    run = {\"test\":[],\"train\":[]}\n",
    "    for i in range(5):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=i)\n",
    "        pl.fit(X_train, y_train[\"party\"])\n",
    "        run[\"test\"].append(pl.score(X_test, y_test[\"party\"]))\n",
    "        run[\"train\"].append(pl.score(X_train, y_train[\"party\"]))\n",
    "\n",
    "    mns = pd.DataFrame(run).mean()\n",
    "    res[\"n\"].append(n)\n",
    "    res[\"test\"].append(mns.loc[\"test\"])\n",
    "    res[\"train\"].append(mns.loc[\"train\"])\n",
    "\n",
    "df = pd.DataFrame(res)\n",
    "df.set_index(\"n\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\"n\":[],\"test\":[],\"train\":[]}\n",
    "for n in np.arange(2,4,0.1):\n",
    "    pl = Pipeline([\n",
    "        ('preproc', preproc),\n",
    "        ('embedding', SVC(C=n)),\n",
    "    ])\n",
    "\n",
    "\n",
    "    run = {\"test\":[],\"train\":[]}\n",
    "    for i in range(5):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=i)\n",
    "        pl.fit(X_train, y_train[\"party\"])\n",
    "        run[\"test\"].append(pl.score(X_test, y_test[\"party\"]))\n",
    "        run[\"train\"].append(pl.score(X_train, y_train[\"party\"]))\n",
    "\n",
    "    mns = pd.DataFrame(run).mean()\n",
    "    res[\"n\"].append(n)\n",
    "    res[\"test\"].append(mns.loc[\"test\"])\n",
    "    res[\"train\"].append(mns.loc[\"train\"])\n",
    "\n",
    "df = pd.DataFrame(res)\n",
    "df.set_index(\"n\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our hyperparameter C is therefore 2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter search for k_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for n in range(2, 17):\n",
    "    res[n] = []\n",
    "    year_prefix_hotencode = Pipeline([\n",
    "        (\"transform-date-to-int\", FunctionTransformer(lambda x : x[\"transaction_date\"].apply(lambda x : time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%d\").timetuple())))),\n",
    "        (\"to-2d-df\", FunctionTransformer(lambda x : pd.DataFrame(x))),\n",
    "        (\"bins\", KBinsDiscretizer(n_bins=n)),\n",
    "    ])\n",
    "    preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('amount', OneHotEncoder(handle_unknown='ignore'), [\"amount\", \"type\", \"ticker\"]),\n",
    "        ('year', year_prefix_hotencode, [\"transaction_date\"]),\n",
    "        ('stdscalerbygroup', StdScalerByGroup(), [\"type\", \"amount\"])\n",
    "    ]\n",
    "    )\n",
    "    pl = Pipeline([\n",
    "        ('preproc', preproc),\n",
    "        ('embedding', SVC(C=3)),\n",
    "    ])\n",
    "    for x in range(25):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y.values.ravel(), random_state=x)\n",
    "        pl.fit(X_train, y_train)\n",
    "        res[n].append(pl.score(X_test, y_test))\n",
    "pd.Series(res).apply(lambda x : np.mean(x)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(res).apply(lambda x : np.mean(x)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our hyperparameter for n_bins is therefore 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "year_prefix_hotencode = Pipeline([\n",
    "    (\"transform-date-to-int\", FunctionTransformer(lambda x : x[\"transaction_date\"].apply(lambda x : time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%d\").timetuple())))),\n",
    "    (\"to-2d-df\", FunctionTransformer(lambda x : pd.DataFrame(x))),\n",
    "    (\"bins\", KBinsDiscretizer(n_bins=14)),\n",
    "])\n",
    "preproc = ColumnTransformer(\n",
    "transformers=[\n",
    "    ('amount', OneHotEncoder(handle_unknown='ignore'), [\"amount\", \"type\", \"ticker\"]),\n",
    "    ('year', year_prefix_hotencode, [\"transaction_date\"]),\n",
    "    ('stdscalerbygroup', StdScalerByGroup(), [\"type\", \"amount\"])\n",
    "]\n",
    ")\n",
    "pl = Pipeline([\n",
    "    ('preproc', preproc),\n",
    "    ('embedding', SVC(C=2.9)),\n",
    "])\n",
    "for x in range(25):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y.values.ravel(), random_state=x)\n",
    "    pl.fit(X_train, y_train)\n",
    "    res.append(pl.score(X_test, y_test))\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\"test\":[],\"train\":[]}\n",
    "for _ in range(100):\n",
    "    pl = Pipeline([\n",
    "        ('preproc', preproc),\n",
    "        ('model', SVC(C=2.9)),\n",
    "    ])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "    pl.fit(X_train, y_train[\"party\"])\n",
    "    res[\"test\"].append(pl.score(X_test, y_test[\"party\"]))\n",
    "    res[\"train\"].append(pl.score(X_train, y_train[\"party\"]))\n",
    "pd.DataFrame(res).plot.hist(bins=np.linspace(0.5, 1, 100),alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks[\"party\"].dropna().value_counts()/len(stocks[\"party\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.drop(X_train.index)[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confusion_matrix(stocks.drop(X_train.index)[\"party\"],pl.predict(stocks.drop(X_train.index).drop(\"party\", axis = 1))))\n",
    "df.columns = [\"DEM_pred\", \"LIB_pred\", \"REP_pred\"] #\n",
    "df.index = [\"DEM_ground\", \"LIB_ground\", \"REP_ground\"] #\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_recall = df[\"DEM_pred\"][\"DEM_ground\"]/(df[\"DEM_pred\"][\"DEM_ground\"]+df[\"REP_pred\"][\"DEM_ground\"])\n",
    "dem_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_recall = df[\"REP_pred\"][\"REP_ground\"]/(df[\"REP_pred\"][\"REP_ground\"]+df[\"DEM_pred\"][\"REP_ground\"])\n",
    "rep_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\"n\":[],\"test\":[],\"train\":[]}\n",
    "for p in np.arange(0.1,1.0,0.05):\n",
    "    pl = Pipeline([\n",
    "        ('preproc', preproc),\n",
    "        ('embedding', SGDClassifier(C=p)),\n",
    "    ])\n",
    "\n",
    "    run = {\"test\":[],\"train\":[]}\n",
    "    for i in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=i)\n",
    "        pl.fit(X_train, y_train[\"party\"])\n",
    "        run[\"test\"].append(pl.score(X_test, y_test[\"party\"]))\n",
    "        run[\"train\"].append(pl.score(X_train, y_train[\"party\"]))\n",
    "\n",
    "    mns = pd.DataFrame(run).mean()\n",
    "    res[\"n\"].append(n)\n",
    "    res[\"test\"].append(mns.loc[\"test\"])\n",
    "    res[\"train\"].append(mns.loc[\"train\"])\n",
    "\n",
    "df = pd.DataFrame(res)\n",
    "df.set_index(\"n\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    \"rep_weight\":[],\n",
    "    \"test\":[],\n",
    "    \"train\":[],\n",
    "    \"rep_recall\":[],\n",
    "    \"dem_recall\":[]\n",
    "}\n",
    "for rep_weight in np.arange(0.1,1,0.05):\n",
    "    pl = Pipeline([\n",
    "        ('preproc', preproc),\n",
    "        ('embedding', SVC(C=3,class_weight={\"DEM\":(1-rep_weight),\"REP\":rep_weight})),\n",
    "    ])\n",
    "\n",
    "    #print(rep_weight)\n",
    "\n",
    "    run = {\"test\":[],\"train\":[],\"rep_recall\":[],\"dem_recall\":[]}\n",
    "    for i in range(5):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=i)\n",
    "        pl.fit(X_train, y_train[\"party\"])\n",
    "        run[\"test\"].append(pl.score(X_test, y_test[\"party\"]))\n",
    "        run[\"train\"].append(pl.score(X_train, y_train[\"party\"]))\n",
    "\n",
    "        mtx = pd.DataFrame(confusion_matrix(stocks.drop(X_train.index)[\"party\"],pl.predict(stocks.drop(X_train.index).drop(\"party\", axis = 1))))\n",
    "        if(len(mtx)==2):\n",
    "            mtx.columns = [\"DEM_pred\", \"REP_pred\"] #\n",
    "            mtx.index = [\"DEM_ground\", \"REP_ground\"] #\n",
    "        else:\n",
    "            mtx.columns = [\"DEM_pred\", \"LIB_pred\", \"REP_pred\"] #\n",
    "            mtx.index = [\"DEM_ground\", \"LIB_ground\", \"REP_ground\"] #\n",
    "\n",
    "\n",
    "        run[\"rep_recall\"].append(mtx[\"REP_pred\"][\"REP_ground\"]/(mtx[\"REP_pred\"][\"REP_ground\"]+mtx[\"DEM_pred\"][\"REP_ground\"]))\n",
    "        run[\"dem_recall\"].append(mtx[\"DEM_pred\"][\"DEM_ground\"]/(mtx[\"DEM_pred\"][\"DEM_ground\"]+mtx[\"REP_pred\"][\"DEM_ground\"]))\n",
    "\n",
    "    mns = pd.DataFrame(run).mean()\n",
    "    res[\"rep_weight\"].append(rep_weight)\n",
    "    res[\"test\"].append(mns.loc[\"test\"])\n",
    "    res[\"train\"].append(mns.loc[\"train\"])\n",
    "    res[\"rep_recall\"].append(mns.loc[\"rep_recall\"])\n",
    "    res[\"dem_recall\"].append(mns.loc[\"dem_recall\"])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(res)\n",
    "df.set_index(\"rep_weight\").drop(\"train\",axis=1).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
